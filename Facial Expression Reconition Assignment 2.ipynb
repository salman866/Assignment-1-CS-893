{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a450ef",
   "metadata": {},
   "source": [
    "## CS 893 Advanced Computer Vision (Assignment No 02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049445b2",
   "metadata": {},
   "source": [
    "This assignment deals with classification of 08 distinct facial expressions along with predicting the dimensional model of affect by assigning corresponding valence and arousal values. The dataset comprises of 287,651 training images and 4999 testing images. The training dataset is a long tail dataset with most of the images belong to happy and neutral classes and very belong to disgust and contempt classes. Moreover, class balanced validation dataset with 100 images from each class has been extracted from training dataset.\n",
    "For feature extraction, two seperate pipelines of convolutional neural network  (VGG and Resnet) has been used with transfer learning. For this learned filter coefficients from VGG-Face have been adopted. The models were retrained on training dataset and multi-output model was compiled having three heads (01 classifier and 02 Regression).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d007f0b",
   "metadata": {},
   "source": [
    "## Required Modules to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ee12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install krippendorff\n",
    "# ! pip install pandas\n",
    "# ! pip install matplotlib\n",
    "# ! pip install tensorflow\n",
    "# ! pip install sklearn\n",
    "# ! pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a64c93",
   "metadata": {},
   "source": [
    "## Import Requisite Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ae2cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, ReLU, Input, ZeroPadding2D, add, concatenate \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, confusion_matrix, auc, classification_report \n",
    "from sklearn.metrics import roc_auc_score, ConfusionMatrixDisplay, average_precision_score, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "import krippendorff # install krippendorff module by executing (pip install krippendorff) \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1597fcfe",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da03589",
   "metadata": {},
   "source": [
    "It is assumed that the dataset has been unzipped and placed in the Dataset folder with following folders inside Dataset folder (test_set and train_set).\n",
    "The dataset (images and innotations) are read and saved in the csv files for test and training data with coulumns (filename, label, valence and arousal).\n",
    "The training dataset is further divided into validation set and training set. The validation set contains 100 images of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2439476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns corresponding image annotation for input image file\n",
    "def GetImageAnnotation(filePath,annotType):\n",
    "    temp1 = filePath.split(\"\\\\\")\n",
    "    temp2 = temp1[3].split(\".\")\n",
    "    if (annotType == 0):\n",
    "        annotationType = \"_exp.npy\"\n",
    "    elif (annotType == 1):\n",
    "        annotationType = \"_val.npy\"\n",
    "    elif (annotType == 2):\n",
    "        annotationType = \"_aro.npy\"\n",
    "    annotationPath = os.path.join(temp1[0],temp1[1],\"annotations\",temp2[0]+annotationType)\n",
    "    annotVal = np.load(annotationPath)\n",
    "    return annotVal\n",
    "# This function extracts annotations of image files\n",
    "def ExtractAnnotations(filePath):\n",
    "    label = GetImageAnnotation(filePath,0)\n",
    "    valence = GetImageAnnotation(filePath,1)\n",
    "    arousal = GetImageAnnotation(filePath,2)\n",
    "    return label, valence, arousal\n",
    "# This function creates a csv file from extracted annotations\n",
    "def CreateCSVFile(imageDir, filename):\n",
    "    fileNamesList = os.listdir(imageDir)\n",
    "    for index, fileName in enumerate(fileNamesList):\n",
    "        filePath = os.path.join(imageDir,fileName)\n",
    "        label, valence, arousal = ExtractAnnotations(filePath)\n",
    "        data1 = pd.DataFrame({'filename': fileName, 'label': [label], 'valence' : [valence], 'arousal' : [arousal]})\n",
    "        if index !=0:\n",
    "            data2 = pd.concat([data2, data1], axis=0)\n",
    "        else:\n",
    "            data2 = data1\n",
    "    data2.to_csv(filename, index = False)\n",
    "# Split training dataset into training and validation dataset    \n",
    "def ExtractValidationset(dataFrame, noOfSamples, noOfClasses):\n",
    "    df = dataFrame.copy()\n",
    "    dfList = []\n",
    "    for i in range(noOfClasses):\n",
    "        tempdf = df.loc[df['label'] == i]\n",
    "        dfClass = tempdf.sample(n=noOfSamples)\n",
    "        dfList.append(dfClass) \n",
    "        if i==0:\n",
    "            extractedDF = dfClass\n",
    "        else:\n",
    "            extractedDF = pd.concat([extractedDF, dfClass], axis=0)\n",
    "        for j in dfClass.index: \n",
    "            df = df.drop(j)\n",
    "     \n",
    "    df.to_csv(r'train_set.csv', index = False)\n",
    "    extractedDF.to_csv(r'val_set.csv', index = False)\n",
    "    print(\"Training and validation dataset files have been saved\")\n",
    "# Calculate class frequency for visualization\n",
    "def CalculateClassFrequency(dataFrame,noOfClasses):\n",
    "    df = dataFrame.copy()\n",
    "    classFrequencyList = []\n",
    "    for i in range(noOfClasses):\n",
    "        tempdf = df.loc[df['label'] == i]\n",
    "        classFrequencyList.append(len(tempdf))\n",
    "    return classFrequencyList\n",
    "# Calculate respective class weights for weighted softmax loss function\n",
    "def CalculateClassWeight(classFrequencyList, beta):\n",
    "    classFrequencyArray = np.array(classFrequencyList)\n",
    "    effectiveNumber = 1.0 - np.power(beta,classFrequencyArray)\n",
    "    weights = (1-beta) / np.array(effectiveNumber)\n",
    "    weights = weights / np.sum(weights) * len(classFrequencyArray)\n",
    "    print(\"Calculated weights are: \" + str(weights))\n",
    "    return weights\n",
    "# This function plots class histogram\n",
    "def PlotClassHistogram(classNameList, classFrequencyList, plotTitle):\n",
    "    plt.bar(classNameList,classFrequencyList)\n",
    "    plt.xlabel(\"Class Names\")\n",
    "    plt.ylabel(\"Number of Images\")\n",
    "    plt.title(\"Image Distribution for \" + plotTitle)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ec2f19",
   "metadata": {},
   "source": [
    "## Run the following block to form csv files for training, validation and test dataset for further processing through dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33adab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "classNameList = np.array(['Neutral','Happy','Sad','Surprise','Fear','Disgust','Anger','Contempt'])\n",
    "imageDirPath = os.path.join('Dataset','train_set','images')\n",
    "CreateCSVFile(imageDirPath, 'train_setComplete.csv')\n",
    "df = pd.read_csv('train_setComplete.csv')\n",
    "ExtractValidationset(df, 100, 8)\n",
    "df = pd.read_csv('train_set.csv')\n",
    "classFrequencyList = CalculateClassFrequency(df,8)\n",
    "classWeights = CalculateClassWeight(classFrequencyList, 0.9999)\n",
    "PlotClassHistogram(classNameList, classFrequencyList, 'Training Dataset')\n",
    "imageDirPath = os.path.join('Dataset','test_set','images')\n",
    "CreateCSVFile(imageDirPath, 'test_set.csv')\n",
    "df = pd.read_csv('test_set.csv')\n",
    "classFrequencyList = CalculateClassFrequency(df,8)\n",
    "PlotClassHistogram(classNameList, classFrequencyList, 'Test Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4237f",
   "metadata": {},
   "source": [
    "## Installing VGG-Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9168a113",
   "metadata": {},
   "source": [
    "Following packages along with their correct versions are required to be installed for using VGG-Face. \n",
    "Remember to revert back to latest versions of tensorflow and keras for running the other parts of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8cc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow==1.14.0\n",
    "# ! pip install keras==2.2.4\n",
    "# ! pip install h5py==2.10.0 --force-reinstall\n",
    "# ! pip install keras_vggface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dedaa9",
   "metadata": {},
   "source": [
    "VGG-Face comes with three pretrained models on face images dataset with backbone networks of (VGG-16, Resnet50 and SEnet50).\n",
    "The models are downloaded and their weights are saved for using them on latest version of tensorflow/keras framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b3d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_vggface.vggface import VGGFace\n",
    "# Based on VGG16 architecture -> old paper(2015)\n",
    "vggfaceModel = VGGFace(model='vgg16')\n",
    "# Based on RESNET50 architecture -> new paper(2017)\n",
    "resnetModel = VGGFace(model='resnet50')\n",
    "# Based on SENET50 architecture -> new paper(2017)\n",
    "senetModel = VGGFace(model='senet50')\n",
    "#Save weights of models\n",
    "vggfaceModel.save_weights('vggface-16.h5')\n",
    "resnetModel.save_weights('vggface-resnet50.h5')\n",
    "senetModel.save_weights('vggface-senet50.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a33a1a5",
   "metadata": {},
   "source": [
    "## Support functions for building VGG-16 based CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe26254",
   "metadata": {},
   "source": [
    "Run this block if VGG-16 based backbone is desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e86d2beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used for adding convolutions layers followed by Maxpool layer\n",
    "def ConvolutionBlock(noOfLayers, noOfFilters, x, convBlockName):\n",
    "    for i in range(noOfLayers):\n",
    "        layerName = convBlockName+'_'+str(i+1)+'_3x3'\n",
    "        x = Conv2D(noOfFilters,(3,3), strides=(1,1),padding='same', activation='relu', name = layerName)(x)\n",
    "    layerName = 'maxPool_'+convBlockName.split('_')[1]+'_'+convBlockName.split('_')[2]\n",
    "    x = MaxPool2D(2,strides=2, name = layerName)(x)\n",
    "    return x\n",
    "# This function cascades Convolutions blocks\n",
    "def BuildVGGLearner(metaParameters,inputImg):\n",
    "    x = inputImg\n",
    "    noOfConvBlocks = metaParameters['noOfConvBlocks']\n",
    "    for i in range(noOfConvBlocks):\n",
    "        noOfLayers = metaParameters['convBlock'+str(i+1)]['noOfLayers']\n",
    "        noOfFilters = metaParameters['convBlock'+str(i+1)]['noOfFilters']\n",
    "        convBlockName = metaParameters['convBlock'+str(i+1)]['name']\n",
    "        x = ConvolutionBlock(noOfLayers,noOfFilters,x,convBlockName)\n",
    "    return x\n",
    "# This function adds a classifier at the end of Convolution blocks\n",
    "def BuildVGGClassifier(metaParameters,inputFeatures):\n",
    "    x = inputFeatures\n",
    "    noOfFCBlocks = metaParameters['noOfFCBlocks']\n",
    "    for i in range(noOfFCBlocks):\n",
    "        noOfNodes = metaParameters['fcBlock'+str(i+1)]['noOfNodes']\n",
    "        activationType = metaParameters['fcBlock'+str(i+1)]['activation']\n",
    "        layerName = metaParameters['fcBlock'+str(i+1)]['name']+'_'+activationType\n",
    "        x = Dense(noOfNodes, activation = activationType, name = layerName)(x)\n",
    "        x = Dropout(0.2, seed = 210)(x)\n",
    "    return x\n",
    "# This function assembles convolution groups and classifier to form a complete VGG-16 model\n",
    "def BuildVGGModel(metaParameters,inputImg):\n",
    "    x = BuildVGGLearner(metaParameters,inputImg)\n",
    "    layerName = 'flattenLayer_VGG'\n",
    "    x = Flatten(name = layerName)(x)\n",
    "    x =  BuildVGGClassifier(metaParameters,x)\n",
    "    model = Model(inputs = inputImg, outputs = x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2128a635",
   "metadata": {},
   "source": [
    "Run the following block to build a VGG-16 model. The softmax layer comprises of 2622 nodes since the VGG-Face (VGG-16) was trained to classify 2622 classes for faces. The architecture of model (no of layers and filters) are defined by a dictionary of metaparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5243f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputImg = Input(shape =(224,224,3), name = 'inputLayer')\n",
    "metaParameters = {'noOfConvBlocks' : 5, \n",
    "                 'convBlock1' : {'name' : 'convBlock_VGG_1','noOfLayers' : 2, 'noOfFilters' : 64},\n",
    "                 'convBlock2' : {'name' : 'convBlock_VGG_2','noOfLayers' : 2, 'noOfFilters' : 128},\n",
    "                 'convBlock3' : {'name' : 'convBlock_VGG_3','noOfLayers' : 3, 'noOfFilters' : 256},\n",
    "                 'convBlock4' : {'name' : 'convBlock_VGG_4','noOfLayers' : 3, 'noOfFilters' : 512},\n",
    "                 'convBlock5' : {'name' : 'convBlock_VGG_5','noOfLayers' : 3, 'noOfFilters' : 512},\n",
    "                 'noOfFCBlocks' : 3, \n",
    "                 'fcBlock1'   : {'name' : 'fcBlock_VGG_1','noOfNodes' : 4096, 'activation' : 'relu'},\n",
    "                 'fcBlock2'   : {'name' : 'fcBlock_VGG_2','noOfNodes' : 4096, 'activation' : 'relu'},\n",
    "                 'fcBlock3'   : {'name' : 'fcBlock_VGG_3','noOfNodes' : 2622, 'activation' : 'softmax'}}\n",
    "model = BuildVGGModel(metaParameters,inputImg)\n",
    "# model.summary()\n",
    "# plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb85dce",
   "metadata": {},
   "source": [
    "## Support functions for building Resnet based CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ff5ef",
   "metadata": {},
   "source": [
    "Run this block for using the Resnet50 based backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70b716c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddConvLayer(noOfFilters,kernel,stride,name,actFlag,inFeature,padding='valid'):\n",
    "    x = Conv2D(noOfFilters,kernel_size=kernel, strides=stride,padding=padding, use_bias=False, name = name)(inFeature)\n",
    "    x = BatchNormalization()(x)\n",
    "    if actFlag:\n",
    "        x = ReLU()(x)\n",
    "    return x\n",
    "    \n",
    "def BuildResnetStem(metaParameters, inputImg):\n",
    "    convBlockName = metaParameters['convBlock1']['name']\n",
    "    noOfFilters = metaParameters['convBlock1']['noOfFilters']\n",
    "    layerName = convBlockName  + '_1_7x7'\n",
    "    x = AddConvLayer(noOfFilters=noOfFilters,kernel=(7,7),stride=(2,2),name=layerName,actFlag=True,inFeature=inputImg,padding='same')\n",
    "    \n",
    "    layerName = 'maxPool_'+convBlockName.split('_')[1]+'_'+convBlockName.split('_')[2]\n",
    "    x = MaxPool2D(pool_size=(3,3), strides=(2,2), name = layerName)(x)\n",
    "    return x\n",
    "\n",
    "def ConvolutionBlock(noOfFilters, x, convBlockName, stride =(2,2)):\n",
    "    layerName = convBlockName + '_1_1x1_proj'\n",
    "    shortcutLayer = AddConvLayer(noOfFilters=4*noOfFilters,kernel=(1,1),stride=stride,name=layerName,actFlag=False,inFeature=x)\n",
    "\n",
    "    layerName = convBlockName + '_1_1x1_reduce'\n",
    "    x = AddConvLayer(noOfFilters=noOfFilters,kernel=(1,1),stride=stride,name=layerName,actFlag=True,inFeature=x)\n",
    "\n",
    "    layerName = convBlockName + '_1_3x3'\n",
    "    x = AddConvLayer(noOfFilters=noOfFilters,kernel=(3,3),stride=(1,1),name=layerName,actFlag=True,inFeature=x,padding='same')\n",
    "\n",
    "    layerName = convBlockName + '_1_1x1_increase'\n",
    "    x = AddConvLayer(noOfFilters=4*noOfFilters,kernel=(1,1),stride=(1,1),name=layerName,actFlag=False,inFeature=x)\n",
    "    layerName = convBlockName+'_Add'\n",
    "    x = add([x, shortcutLayer], name = layerName)\n",
    "    x = ReLU()(x)\n",
    "    return x\n",
    "\n",
    "def BottleneckBlock(noOfFilters, x, convBlockName, bottleneckBlockNo):\n",
    "    shortcutLayer = x\n",
    "    layerName = convBlockName+'_'+str(bottleneckBlockNo)+'_1x1_reduce'\n",
    "    x = AddConvLayer(noOfFilters=noOfFilters,kernel=(1,1),stride=(1,1),name=layerName,actFlag=True,inFeature=x)\n",
    "    \n",
    "    layerName = convBlockName+'_'+str(bottleneckBlockNo)+'_3x3'\n",
    "    x = AddConvLayer(noOfFilters=noOfFilters,kernel=(3,3),stride=(1,1),name=layerName,actFlag=True,inFeature=x,padding='same')\n",
    "\n",
    "    layerName = convBlockName+'_'+str(bottleneckBlockNo)+'_1x1_increase'\n",
    "    x = AddConvLayer(noOfFilters=4*noOfFilters,kernel=(1,1),stride=(1,1),name=layerName,actFlag=False,inFeature=x)\n",
    "    \n",
    "    layerName = 'BottleneckBlock_'+convBlockName.split('_')[2]+'_'+str(bottleneckBlockNo)+'_Add'\n",
    "    x = add([shortcutLayer, x], name=layerName)\n",
    "    x = ReLU()(x)\n",
    "    return x \n",
    "\n",
    "def BuildResnetClassifier(metaParameters,inputFeatures):\n",
    "    x = inputFeatures\n",
    "    noOfFCBlocks = metaParameters['noOfFCBlocks']\n",
    "    for i in range(noOfFCBlocks):\n",
    "        noOfNodes = metaParameters['fcBlock'+str(i+1)]['noOfNodes']\n",
    "        activationType = metaParameters['fcBlock'+str(i+1)]['activation']\n",
    "        layerName = metaParameters['fcBlock'+str(i+1)]['name'] +'_'+activationType\n",
    "        x = Dense(noOfNodes, activation = activationType, name = layerName)(x)\n",
    "        x = Dropout(0.3, seed = 210)(x)\n",
    "    return x\n",
    "\n",
    "def BuildResnetLearner(metaParameters,x):\n",
    "    noOfConvBlocks = metaParameters['noOfConvBlocks']\n",
    "    for i in range(2,noOfConvBlocks+1):\n",
    "        convBlockName = metaParameters['convBlock'+str(i)]['name']\n",
    "        noOfFilters = metaParameters['convBlock'+str(i)]['noOfFilters']\n",
    "        noOfBottleneckBlocks = metaParameters['convBlock'+str(i)]['noOfBottleneckBlocks']\n",
    "        if i == 2:\n",
    "            x = ConvolutionBlock(noOfFilters, x, convBlockName, stride =(1,1))\n",
    "        else:\n",
    "            x = ConvolutionBlock(noOfFilters, x, convBlockName)\n",
    "\n",
    "        for i in range(noOfBottleneckBlocks):\n",
    "            x = BottleneckBlock(noOfFilters, x, convBlockName, i+2)\n",
    "    return x\n",
    "\n",
    "def BuildResnetModel(metaParameters,inputImg):\n",
    "    x = BuildResnetStem(metaParameters,inputImg)\n",
    "    x = BuildResnetLearner(metaParameters,x)\n",
    "    layerName = 'globalAveragePooling_Resnet'\n",
    "    x = GlobalAveragePooling2D(name=layerName)(x)\n",
    "    x = BuildResnetClassifier(metaParameters,x)\n",
    "    model = Model(inputs = inputImg, outputs = x)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5d513",
   "metadata": {},
   "source": [
    "Run the following block to build a Resnet50 model. The softmax layer comprises of 8631 nodes since the VGG-Face (Resnet50) was trained to classify 8631 classes for faces. The architecture of model (no of layers and filters) are defined by a dictionary of metaparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57ad71a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "layerName = 'inputLayer'\n",
    "inputImg = Input(shape =(224,224,3),name=layerName)\n",
    "metaParameters = {'noOfConvBlocks' : 5, \n",
    "                 'convBlock1' : {'name' : 'convBlock_Resnet_1','noOfBottleneckBlocks' : 0, 'noOfFilters' : 64},\n",
    "                 'convBlock2' : {'name' : 'convBlock_Resnet_2','noOfBottleneckBlocks' : 2, 'noOfFilters' : 64},\n",
    "                 'convBlock3' : {'name' : 'convBlock_Resnet_3','noOfBottleneckBlocks' : 3, 'noOfFilters' : 128},\n",
    "                 'convBlock4' : {'name' : 'convBlock_Resnet_4','noOfBottleneckBlocks' : 5, 'noOfFilters' : 256},\n",
    "                 'convBlock5' : {'name' : 'convBlock_Resnet_5','noOfBottleneckBlocks' : 2, 'noOfFilters' : 512},\n",
    "                 'noOfFCBlocks' : 1, \n",
    "                 'fcBlock1'   : {'name' : 'fcBlock_Resnet_1','noOfNodes' : 8631, 'activation' : 'softmax'}}\n",
    "model = BuildResnetModel(metaParameters,inputImg)\n",
    "# plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b040d6",
   "metadata": {},
   "source": [
    "## Support Functions for Facial Expression classier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b44321c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used for defining a weighted softmax loss function for handing long tail dataset.\n",
    "def WeightedSoftmax(y_true, y_pred):\n",
    "    class_weight = tf.convert_to_tensor(np.array([0.57261169,0.57229192,0.62097051,0.7573698,1.21360446,1.80900564,0.62412396,1.83002202]),dtype=tf.float32)\n",
    "    unreduced_scce = tf.keras.losses.SparseCategoricalCrossentropy(reduction = tf.keras.losses.Reduction.NONE)\n",
    "    loss = unreduced_scce(y_true, y_pred)\n",
    "    weight_mask = tf.gather(class_weight, y_true)\n",
    "    loss = tf.math.multiply(loss, weight_mask)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "# This function adds a regression block by cascading dense layers followed by a CNN\n",
    "def BuildRegressionBlock(metaParameters,inputFeatures, outputType):\n",
    "    x = inputFeatures\n",
    "    noOfRegressionBlocks = metaParameters['noOfRegressionBlocks']\n",
    "    for i in range(noOfRegressionBlocks):\n",
    "        noOfNodes = metaParameters['fcBlock'+str(i+1)]['noOfNodes']\n",
    "        activationType = metaParameters['fcBlock'+str(i+1)]['activation']\n",
    "        layerName = metaParameters['fcBlock'+str(i+1)]['name']+outputType+'_'+activationType\n",
    "        x = Dense(noOfNodes, activation = activationType, name = layerName)(x)\n",
    "        x = Dropout(0.2, seed = 210)(x)\n",
    "    return x\n",
    "# This function load the saved weights and extracts the feature descriptor layers of VGG-Face and make only the desired layers trainable\n",
    "def ExtractFaceDescriptorModel(model,LastExtractedLayer, LastTrainableLayer,modelWeightPath):\n",
    "    model.load_weights(modelWeightPath)\n",
    "    faceDescriptorModel = Model(inputs=model.layers[0].input, outputs=model.get_layer(LastExtractedLayer).output)\n",
    "    # vgg_face_descriptor.summary()\n",
    "    for layer in faceDescriptorModel.layers:\n",
    "        if (layer.name == LastTrainableLayer): break\n",
    "        layer.trainable = False\n",
    "    return faceDescriptorModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624e4a4c",
   "metadata": {},
   "source": [
    "Run this block to use VGG-16 based backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f329fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function assembles convolution groups and classifier to form a complete Facial Expression Recognition model with multi-output\n",
    "def BuildFERModel(model,metaParameters):\n",
    "    inputLayer = model.layers[0].input\n",
    "    x = model.layers[-1].output\n",
    "    layerName = 'globalAveragePooling_FER'\n",
    "    x = GlobalAveragePooling2D(name=layerName)(x)\n",
    "    classificationOutput = BuildVGGClassifier(metaParametersFER,x)\n",
    "    classificationOutput = Dense(8, activation = 'softmax', name = 'classification_output')(classificationOutput)\n",
    "    valenceOutput = BuildRegressionBlock(metaParametersFER,x, 'valence')\n",
    "    valenceOutput = Dense(1, activation = 'tanh', name = 'valence_output')(valenceOutput)\n",
    "    arousalOutput = BuildRegressionBlock(metaParametersFER,x, 'arousal')\n",
    "    arousalOutput = Dense(1, activation = 'tanh', name = 'arousal_output')(arousalOutput)\n",
    "    # model = Model(inputs = [inputLayer,landmarks], outputs = [classificationOutput, valenceOutput, arousalOutput])\n",
    "    model = Model(inputs = inputLayer, outputs = [classificationOutput, valenceOutput, arousalOutput])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b9dc0e",
   "metadata": {},
   "source": [
    "Run this block for using Resnet50 based backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cbbe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildFERModel(model,metaParameters):\n",
    "    inputLayer = model.layers[0].input\n",
    "    x = model.layers[-2].output\n",
    "    x = Conv2D(1024,kernel_size=(1,1), strides=(1,1), activation='relu')(x)\n",
    "    layerName = 'globalAveragePooling_FER'\n",
    "    x = GlobalAveragePooling2D(name=layerName)(x)\n",
    "    classificationOutput = BuildResnetClassifier(metaParameters,x)\n",
    "    classificationOutput = Dense(8, activation = 'softmax', name = 'classification_output')(classificationOutput)\n",
    "    valenceOutput = BuildRegressionBlock(metaParametersFER,x, 'valence')\n",
    "    valenceOutput = Dense(1, activation = 'tanh', name = 'valence_output')(valenceOutput)\n",
    "    arousalOutput = BuildRegressionBlock(metaParametersFER,x, 'arousal')\n",
    "    arousalOutput = Dense(1, activation = 'tanh', name = 'arousal_output')(arousalOutput)\n",
    "    # model = Model(inputs = [inputLayer,landmarks], outputs = [classificationOutput, valenceOutput, arousalOutput])\n",
    "    model = Model(inputs = inputLayer, outputs = [classificationOutput, valenceOutput, arousalOutput])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d3fad",
   "metadata": {},
   "source": [
    "## Load saved weights to the VGG-Face (VGG-16) model and extract desired layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf032b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelWeightPath = 'vggface-16.h5'\n",
    "LastExtractedLayer = 'maxPool_VGG_5'\n",
    "LastTrainableLayer = 'maxPool_VGG_4'\n",
    "faceDescriptorModel = ExtractFaceDescriptorModel(model,LastExtractedLayer, LastTrainableLayer,modelWeightPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac489c47",
   "metadata": {},
   "source": [
    "Run the following block to build a custom model for facial expression recognition with VGG-16 backbone. The softmax layer comprises of 8 nodes to classify 08 facial expressions. Moreover, the model also have two regression heads for predicting valence and arousal.\n",
    "Since the dataset is a long tail dataset, a custom loss function (weighted softmax loss) has been used to give more priority to less represented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd9bf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "metaParametersFER = {'noOfFCBlocks' : 2, \n",
    "                 'fcBlock1'   : {'name' : 'fcBlock_FER_1','noOfNodes' : 512, 'activation' : 'relu'},\n",
    "                 'fcBlock2'   : {'name' : 'fcBlock_FER_2','noOfNodes' : 512, 'activation' : 'relu'},\n",
    "                 'noOfRegressionBlocks' : 2, \n",
    "                 'fcBlockReg1'   : {'name' : 'fcBlock_Reg_1','noOfNodes' : 512, 'activation' : 'relu'},\n",
    "                 'fcBlockReg2'   : {'name' : 'fcBlock_Reg_2','noOfNodes' : 512, 'activation' : 'relu'}}\n",
    "\n",
    "model = BuildFERModel(model = faceDescriptorModel,metaParameters = metaParametersFER)\n",
    "model.compile(optimizer='adam',loss={'classification_output': WeightedSoftmax,'valence_output': 'mse', 'arousal_output': 'mse'}, \n",
    "              loss_weights={'classification_output': 1.0, 'valence_output': 0.25, 'arousal_output': 0.25}, \n",
    "              metrics = {'classification_output': 'accuracy', 'valence_output': 'mse', 'arousal_output' : 'mse'})\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533be2cb",
   "metadata": {},
   "source": [
    "## Load saved weights to the VGG-Face (Resnet50) model and extract desired layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8f3935",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelWeightPath = 'vggface-resnet50.h5'\n",
    "LastExtractedLayer = 'globalAveragePooling_Resnet'\n",
    "LastTrainableLayer = 'BottleneckBlock_3_4_Add'\n",
    "faceDescriptorModel = ExtractFaceDescriptorModel(model,LastExtractedLayer, LastTrainableLayer,modelWeightPath)\n",
    "# faceDescriptorModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63869ec6",
   "metadata": {},
   "source": [
    "Run the following block to build a custom model for facial expression recognition using Resnet50 backbone. The softmax layer comprises of 8 nodes to classify 08 facial expressions. Moreover, the model also have two regression heads for predicting valence and arousal. Since the dataset is a long tail dataset, a custom loss function (weighted softmax loss) has been used to give more priority to less represented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68218552",
   "metadata": {},
   "outputs": [],
   "source": [
    "metaParametersFER = {'noOfFCBlocks' : 2, \n",
    "                 'fcBlock1'   : {'name' : 'fcBlock_FER_1','noOfNodes' : 512, 'activation' : 'relu'},\n",
    "                 'fcBlock2'   : {'name' : 'fcBlock_FER_2','noOfNodes' : 512, 'activation' : 'relu'},\n",
    "                 'noOfRegressionBlocks' : 2, \n",
    "                 'fcBlockReg1'   : {'name' : 'fcBlock_Reg_1','noOfNodes' : 512, 'activation' : 'relu'},\n",
    "                 'fcBlockReg2'   : {'name' : 'fcBlock_Reg_2','noOfNodes' : 512, 'activation' : 'relu'}}\n",
    "\n",
    "model = BuildFERModel(faceDescriptorModel,metaParametersFER)\n",
    "model.compile(optimizer='adam',loss={'classification_output': WeightedSoftmax,'valence_output': 'mse', 'arousal_output': 'mse'}, \n",
    "              loss_weights={'classification_output': 1.0, 'valence_output': 0.25, 'arousal_output': 0.25}, \n",
    "              metrics = {'classification_output': 'accuracy', 'valence_output': 'mse', 'arousal_output' : 'mse'})\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed585a",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4169cf32",
   "metadata": {},
   "source": [
    "Run the following block to train the model. Here 'flow_from_dataframe' has been used to send image batches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48039391",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint ( 'FER-Checkpoint.h5' , save_best_only = True , monitor = 'val_loss' )\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', patience = 4)\n",
    "train_dir = os.path.join('Dataset','train_set','images')\n",
    "dfTrain=pd.read_csv(r\"train_set.csv\")\n",
    "dfVal=pd.read_csv(r\"val_set.csv\")\n",
    "datagenTrain=ImageDataGenerator(rescale=1./255, rotation_range=10, zoom_range=0.2, brightness_range=[0.8,1.2], fill_mode='nearest')\n",
    "datagenVal=ImageDataGenerator(rescale=1./255)\n",
    "train_generator=datagenTrain.flow_from_dataframe(dataframe=dfTrain, directory=train_dir, x_col=\"filename\", \n",
    "                        y_col=[\"label\",\"valence\",\"arousal\"], class_mode=\"multi_output\", target_size=(224,224), batch_size=64, seed=210)\n",
    "val_generator=datagenVal.flow_from_dataframe(dataframe=dfVal, directory=train_dir, x_col=\"filename\", \n",
    "                      y_col=[\"label\",\"valence\",\"arousal\"], class_mode=\"multi_output\", target_size=(224,224), batch_size=64, shuffle = False)\n",
    "history = model.fit(train_generator,epochs=31, validation_data= val_generator, callbacks =[checkpoint,earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5cc0c9",
   "metadata": {},
   "source": [
    "## Support Functions for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cb8336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used for plotting the training progress of the model\n",
    "def PlotTrainingGraph(history, graphType):\n",
    "    trainingValue = history.history[graphType]\n",
    "    validationValue = history.history['val_'+graphType]\n",
    "    plot_epochs = range(1, len(trainingValue)+1)\n",
    "    if (len(graphType.split('_')) == 3):\n",
    "        temp1 = graphType.split('_')[0]\n",
    "        temp2 = graphType.split('_')[2]\n",
    "    else:\n",
    "        temp1 = graphType\n",
    "        temp2 = 'Cumulative'\n",
    "    plt.plot(plot_epochs, trainingValue, 'r', label='Training')\n",
    "    plt.plot(plot_epochs, validationValue, 'b', label='Validation')\n",
    "    plt.title(temp1+' '+temp2)\n",
    "    plt.ylabel(temp2)  #Y-axis label\n",
    "    plt.xlabel('epoch')  #X-axis label\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "# This function is used for printing Confusion Matrix     \n",
    "def DisplayConfusions(labelsY, predictions,class_names):\n",
    "    confMat = confusion_matrix(labelsY, predictions)\n",
    "    print(confMat)\n",
    "    plt.rcParams[\"figure.figsize\"] = (14,7)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=confMat, display_labels=class_names)\n",
    "    \n",
    "    disp.plot(xticks_rotation = 90)\n",
    "    plt.show()\n",
    "# This function is used for calculating concordance correlation coefficient    \n",
    "def ConCorelationCoef(groundTruth, predicted):\n",
    "    # covariance between groundTruth and predicted\n",
    "    N = len(predicted)\n",
    "    s_xy = 1.0 / (N - 1.0) * np.sum((groundTruth - np.mean(groundTruth)) * (predicted - np.mean(predicted)))\n",
    "    # means\n",
    "    x_m = np.mean(groundTruth)\n",
    "    y_m = np.mean(predicted)\n",
    "    # variances\n",
    "    s_x_sq = np.var(groundTruth)\n",
    "    s_y_sq = np.var(predicted)\n",
    "    # condordance correlation coefficient\n",
    "    ccc = (2.0*s_xy) / (s_x_sq + s_y_sq + (x_m-y_m)**2)\n",
    "    return ccc\n",
    "# This function is used for calculating the Sign Agreement score (SAGR)\n",
    "def SAGRScore(groundTruth, predicted):\n",
    "    prod = np.multiply(groundTruth, predicted)\n",
    "    prod[prod < 0] = 0\n",
    "    prod[prod > 0] = 1\n",
    "    sagr = (1/len(prod))*sum(prod)\n",
    "    return sagr\n",
    "# This function is used for extracting ground truth values from dataset\n",
    "def ExtractGroundTruth(groundTruthType,groundTruthFile):\n",
    "    df = pd.read_csv(groundTruthFile, index_col=False)\n",
    "    return df[groundTruthType].to_numpy()\n",
    "#This function display images    \n",
    "def DisplayImages(indexes, filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    filenames = df['filename']\n",
    "    \n",
    "    \n",
    "    for i in range(len(index)):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        img = cv2.cvtColor(datasetX[index[i]], cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img)\n",
    "        plt.title(predictions[index[i]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b47aa1e",
   "metadata": {},
   "source": [
    "## Plot training and validation graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b18a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotTrainingGraph(history, 'classification_output_accuracy')\n",
    "PlotTrainingGraph(history, 'loss')\n",
    "PlotTrainingGraph(history, 'classification_output_loss')\n",
    "PlotTrainingGraph(history, 'valence_output_mse')\n",
    "PlotTrainingGraph(history, 'arousal_output_mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9bc3dd",
   "metadata": {},
   "source": [
    "## Predict on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c034b517",
   "metadata": {},
   "source": [
    "Run following cell to make predictions on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b162488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3999 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "test_dir = os.path.join('Dataset','val_set','images')\n",
    "dfTest=pd.read_csv(r\"test_set.csv\")\n",
    "datagen=ImageDataGenerator(rescale=1./255)\n",
    "test_generator=datagen.flow_from_dataframe(dataframe=dfTest, directory=test_dir, x_col=\"filename\", \n",
    "                        y_col=[\"label\",\"valence\",\"arousal\"], class_mode=\"multi_output\", target_size=(224,224), batch_size=64, shuffle=False)\n",
    "predictions = model.predict(test_generator,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2da49",
   "metadata": {},
   "source": [
    "## Extract predicted (labels/values) and ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0176ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedLabels = np.argmax(predictions[0], axis=-1)\n",
    "predictedLabelsProb = predictions[0]\n",
    "groundTruthLabels = ExtractGroundTruth('label','test_set.csv')\n",
    "groundTruthLabelsHotencoded = to_categorical(groundTruthLabels, 8)\n",
    "groundTruthLabels = groundTruthLabels.reshape(groundTruthLabels.shape[0],1)\n",
    "predictedLabels = predictedLabels.reshape(predictedLabels.shape[0],1)\n",
    "valencePredicted = predictions[1]\n",
    "arousalPredicted = predictions[2]\n",
    "valencePredicted = np.squeeze(np.asarray(valencePredicted))\n",
    "arousalPredicted = np.squeeze(np.asarray(arousalPredicted))\n",
    "valenceGroundTruth = ExtractGroundTruth('valence','test_set.csv')\n",
    "arousalGroundTruth = ExtractGroundTruth('arousal','test_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c2404",
   "metadata": {},
   "source": [
    "## Extract metrics for test dataset evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "388337c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"Neutral\", \"Happy\", \"Sad\", \"Surprise\", \"Fear\", \"Disgust\", \"Anger\", \"Contempt\"]\n",
    "accuracy = accuracy_score(groundTruthLabels, predictedLabels)\n",
    "kappa = cohen_kappa_score(groundTruthLabels, predictedLabels)\n",
    "confMatrix = confusion_matrix(groundTruthLabels, predictedLabels)\n",
    "classificationReport = classification_report(groundTruthLabels,predictedLabels,target_names=class_names)\n",
    "roc_auc = roc_auc_score(groundTruthLabelsHotencoded, predictedLabelsProb)\n",
    "aucPR = average_precision_score(groundTruthLabelsHotencoded, predictedLabelsProb)\n",
    "\n",
    "print('Accuracy of the trained model is '+str(accuracy))\n",
    "print('Cohens Kappa of the trained model is '+str(kappa))\n",
    "print('Area under the ROC of the trained model is '+str(roc_auc))\n",
    "print('Area under the Precision-Recall curve of the trained model is '+str(aucPR))\n",
    "print(classificationReport)\n",
    "# print(confMatrix)\n",
    "DisplayConfusions(groundTruthLabels,predictedLabels,class_names)\n",
    "\n",
    "print(\"----------Regression metrics are depicted below----------\")\n",
    "rmseValence = mean_squared_error(valenceGroundTruth, valencePredicted, squared=False)\n",
    "rmseArousal = mean_squared_error(arousalGroundTruth, arousalPredicted, squared=False)\n",
    "corrValence, _ = pearsonr(valenceGroundTruth, valencePredicted)\n",
    "corrArousal, _ = pearsonr(arousalGroundTruth, arousalPredicted)\n",
    "valenceCCC = ConCorelationCoef(valenceGroundTruth, valencePredicted)\n",
    "arousalCCC = ConCorelationCoef(arousalGroundTruth, arousalPredicted)\n",
    "valenceSagr = SAGRScore(valenceGroundTruth, valencePredicted)\n",
    "arousalSagr = SAGRScore(arousalGroundTruth, arousalPredicted)\n",
    "\n",
    "print('Root mean squared error for valence is '+str(rmseValence))\n",
    "print('Root mean squared error for arousal is '+str(rmseArousal))\n",
    "print('Correlation for valence is '+str(corrValence))\n",
    "print('Correlation for arousal is '+str(corrArousal))\n",
    "print('Concordance Correlation Coefficient for valence is '+str(valenceCCC))\n",
    "print('Concordance Correlation Coefficient for arousal is '+str(arousalCCC))\n",
    "print('Sign Agreement score for valence is '+str(valenceSagr))\n",
    "print('Sign Agreement score for for arousal is '+str(arousalSagr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3827e960",
   "metadata": {},
   "source": [
    "## Display Wrongly Classified Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a2f8ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DisplayImages(indexes, filename, groundTruthLabels,predictedLabels):\n",
    "    class_names = {0:\"Neutral\", 1:\"Happy\", 2:\"Sad\", 3:\"Surprise\", 4:\"Fear\", 5:\"Disgust\", 6:\"Anger\", 7:\"Contempt\"}\n",
    "    df = pd.read_csv(filename)\n",
    "    filenames = df['filename']\n",
    "    j=0\n",
    "    for i, index in enumerate(indexes):\n",
    "        if i%35 == 0:\n",
    "            j=j+1\n",
    "            if j>15: break\n",
    "            imageName = filenames.loc[index]\n",
    "            imageFilePath = os.path.join('Dataset','val_set','images',imageName)\n",
    "            plt.subplot(5,3,j)\n",
    "            img = cv2.imread(imageFilePath)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            plt.imshow(img)\n",
    "            plt.title('Pred: ' + class_names[predictedLabels[index][0]] + ', GT: ' + class_names[groundTruthLabels[index][0]])\n",
    "            frame1 = plt.gca()\n",
    "            frame1.axes.get_xaxis().set_visible(False)\n",
    "            frame1.axes.get_yaxis().set_visible(False)\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02be4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = groundTruthLabels==predictedLabels\n",
    "b = np.where(a==True)\n",
    "wronglyClassifiedIndexes = b[0]\n",
    "DisplayImages(wronglyClassifiedIndexes, 'test_set.csv', groundTruthLabels, predictedLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f509f0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
