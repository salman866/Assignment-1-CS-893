{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d47191",
   "metadata": {},
   "source": [
    "# CS 893 Advanced Computer Vision (Assignment No 01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9192d452",
   "metadata": {},
   "source": [
    "This assignment deals with classification of 10 different traffic signs from dataset which is a subset of Belgium TSC. Two different approaches have been implemented for this classification task. For both implemented algorithms, a pipeline of Computer Vision and Machine Learning algorithms have been used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87633222",
   "metadata": {},
   "source": [
    "# Approach 1 (Key Point Feature Extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2af382a",
   "metadata": {},
   "source": [
    "Important points of this approach are following:-\n",
    "1. Read and store the Image dataset\n",
    "    i.Convert images into grayscale\n",
    "    ii.Resize images into 128x128 dimensions\n",
    "2. Extract keypoints and feature vectors using SIFT\n",
    "3. Segregating these feature vectors into clusters using Kmeans \n",
    "4. Assigning feature vectors to respective clusters to form bag of visual words\n",
    "5. Apply feature scaling\n",
    "6. Training a multiclass SVM classifier with bag of visual words\n",
    "7. Making predictions of test dataset and calculating metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b35117",
   "metadata": {},
   "source": [
    "# Approach 2 (Histogram of Gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc4be04",
   "metadata": {},
   "source": [
    "Important points of this approach are following:-\n",
    "1. Read and store the Image dataset\n",
    "    i.Convert images into grayscale\n",
    "    ii.Resize images into 128x128 dimensions\n",
    "2. Extract feature vectors through Histogram of Gradients on the grayscale image\n",
    "3. Apply feature scaling\n",
    "4. Apply PCA on scaled features for dimentionality reduction\n",
    "5. Training a multiclass SVM classifier with the HOG features (scaled and reduced)\n",
    "6. Making predictions of test dataset and calculating metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234edd38",
   "metadata": {},
   "source": [
    "# Install requisite packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54893b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip uninstall opencv-python -y\n",
    "! pip uninstall opencv-contrib-python -y\n",
    "! pip install opencv-python==3.4.11.45\n",
    "! pip install opencv-contrib-python==3.4.11.45\n",
    "! pip install scikit-learn\n",
    "! pip install scikit-image\n",
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d4e3d",
   "metadata": {},
   "source": [
    "# Import Requisite Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e3f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from skimage.feature import hog\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "from skimage import io\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb8b96e",
   "metadata": {},
   "source": [
    "# Supporting Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2bac98",
   "metadata": {},
   "source": [
    "Number of supporting functions have been implemented that are required to successfully train and test both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3ff06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the image class name based on the folder name\n",
    "def FindClass(folderName):\n",
    "    if folderName == \"00001\":\n",
    "        label = \"Hump\"\n",
    "    elif folderName == \"00002\":\n",
    "        label = \"Limit 70\"\n",
    "    elif folderName == \"00003\":\n",
    "        label = \"Limit 50\"\n",
    "    elif folderName == \"00004\":\n",
    "        label = \"Children crossing\"\n",
    "    elif folderName == \"00005\":\n",
    "        label = \"Forbidden Direction\"\n",
    "    elif folderName == \"00006\":\n",
    "        label = \"Give way\"\n",
    "    elif folderName == \"00007\":\n",
    "        label = \"No entry\"\n",
    "    elif folderName == \"00008\":\n",
    "        label = \"Bicycle way\"\n",
    "    elif folderName == \"00009\":\n",
    "        label = \"Bicyle & children way\"\n",
    "    else:\n",
    "        label = \"No Parking\"\n",
    "    return label\n",
    "# This function is used for transforming the dataset (Grayscale,Resize) into required format for further processing \n",
    "def BuildDataset(datasetDirectoryName, trainFlag = True):\n",
    "    datasetX = []\n",
    "    labelsY = []\n",
    "    folderNamesList = os.listdir(datasetDirectoryName)\n",
    "    # Loop through all the subfolders of Dataset\n",
    "    for index,folderName in enumerate(folderNamesList):\n",
    "        imgClassFolderPath = os.path.join(datasetDirectoryName, folderName)\n",
    "        if trainFlag:\n",
    "            filePath = os.path.join(imgClassFolderPath, 'train.txt')\n",
    "        else:\n",
    "            filePath = os.path.join(imgClassFolderPath, 'test.txt')\n",
    "\n",
    "        file = open (filePath, 'r')\n",
    "        readFile = file.readlines() \n",
    "       #loop through all the image files for training/testing in the subfolder (Individual classes)\n",
    "        for i in range(len(readFile)):\n",
    "            imgPath = os.path.join(imgClassFolderPath, readFile[i].rstrip(\"\\n\"))\n",
    "            img = cv2.imread(imgPath)\n",
    "            imgGray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # convert image to grayscale\n",
    "            imgGray = cv2.resize(imgGray, (128,128))\n",
    "            datasetX.append(imgGray)\n",
    "            label = FindClass(folderName)\n",
    "            labelsY.append(label) \n",
    "    return datasetX, labelsY\n",
    "\n",
    "def ReadDataset(datasetDirectoryName, trainFlag = True):\n",
    "    datasetX = []\n",
    "    labelsY = []\n",
    "    folderNamesList = os.listdir(datasetDirectoryName)\n",
    "    # Loop through all the subfolders of Dataset\n",
    "    for index,folderName in enumerate(folderNamesList):\n",
    "        imgClassFolderPath = os.path.join(datasetDirectoryName, folderName)\n",
    "        if trainFlag:\n",
    "            filePath = os.path.join(imgClassFolderPath, 'train.txt')\n",
    "        else:\n",
    "            filePath = os.path.join(imgClassFolderPath, 'test.txt')\n",
    "\n",
    "        file = open (filePath, 'r')\n",
    "        readFile = file.readlines() \n",
    "       #loop through all the image files for training/testing in the subfolder (Individual classes)\n",
    "        for i in range(len(readFile)):\n",
    "            imgPath = os.path.join(imgClassFolderPath, readFile[i].rstrip(\"\\n\"))\n",
    "            img = cv2.imread(imgPath)\n",
    "            datasetX.append(img)\n",
    "            label = FindClass(folderName)\n",
    "            labelsY.append(label) \n",
    "    return datasetX, labelsY\n",
    "\n",
    "# This function is used for extracting features from images \n",
    "def ExtractFeatures(featureExtractorType, img):\n",
    "    if featureExtractorType == \"SIFT\":\n",
    "        sift = cv2.xfeatures2d.SIFT_create()\n",
    "        keyPoint, featDes = sift.detectAndCompute(img, None)\n",
    "    elif featureExtractorType == \"HOG\":\n",
    "        featDes = hog(img, orientations=9, pixels_per_cell=(32, 32), cells_per_block=(4, 4), block_norm = 'L2-Hys')\n",
    "        featDes = np.hstack(featDes)\n",
    "    return featDes\n",
    "# This function returns feature vector length based of feature extractor type\n",
    "def GetFeatureVectorLength(featureExtractorType):\n",
    "    if featureExtractorType == \"SIFT\":\n",
    "        featureVectorLength = 128\n",
    "    elif featureExtractorType == \"HOG\":\n",
    "        featureVectorLength = 1\n",
    "    return featureVectorLength\n",
    "# This function is used for stacking features vertically by opening the feature descriptor list along its second dimension\n",
    "def StackFeatureDescriptors(featDescList):\n",
    "    featDescriptorStack = []\n",
    "    for i in range(len(featDescList)):\n",
    "        featDescriptorStack[i*featDescList[i].shape[0]:featDescList[i].shape[0]-1] = featDescList[i]\n",
    "    featDescriptorStack = np.array(featDescriptorStack)\n",
    "    return featDescriptorStack\n",
    "# This function builds clusters using Kmeans clustering \n",
    "def ClusterofFeatures(featDescriptorStack, noOfClusters):\n",
    "    kmeans = KMeans(n_clusters = noOfClusters, max_iter = 4000, random_state=15)\n",
    "    kmeans.fit(featDescriptorStack)\n",
    "    return kmeans\n",
    "# This function assigns each feature vector from dataset to the clusters (formed during Kmeans cluster building step)\n",
    "# Bag of Visual Words are created by passing feature vectors from dataset \n",
    "# Similar features clustered together\n",
    "# Each cluster represents a seperate feature type (Visual Word)\n",
    "def ConstructBOVW(kmeans, featureDescriptorList, datasetLength, noOfClusters, featureVectorLength):\n",
    "    imgFeatures = np.zeros((datasetLength,noOfClusters))\n",
    "    for i in range(datasetLength):\n",
    "        for j in range(len(featureDescriptorList[i])):\n",
    "            feature = featureDescriptorList[i][j].reshape(1,featureVectorLength)\n",
    "            clusterNo = kmeans.predict(feature)\n",
    "            imgFeatures[i][clusterNo] += 1\n",
    "    return imgFeatures  \n",
    "# This function is used for plotting the histogram of features in a particular cluster\n",
    "def PlotHistogram(imgFeatures, noOfClusters):\n",
    "    clusterAxis = np.arange(1,noOfClusters+1,1)\n",
    "    featureAxis = np.sum(imgFeatures,axis=0)\n",
    "    plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "    plt.bar(clusterAxis, featureAxis)\n",
    "    plt.xlabel(\"Cluster Index\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Histogram of features for the Dataset\")\n",
    "    plt.show()\n",
    "#This function scales the features by removing the mean = 0 and scaling to unit variance\n",
    "def ScaleFeatures(imgFeatures):\n",
    "    scaleObj = StandardScaler().fit(imgFeatures)        \n",
    "    scaledImgFeatures = scaleObj.transform(imgFeatures)\n",
    "    return scaleObj, scaledImgFeatures\n",
    "#This function applies Principal Component Analysis for dimensionality reduction of features\n",
    "def ApplyPCA(imgFeatures):\n",
    "    pcaObj = PCA(n_components=50)\n",
    "    pcaFeatures = pcaObj.fit_transform(imgFeatures)\n",
    "    return pcaObj, pcaFeatures\n",
    "#This function selects the optimized hyperparameters for State Vector Machine\n",
    "def SelectHyperparametersSVM(dataX,labelY):\n",
    "    print(\"Starting search for SVM hyperparameters....\")\n",
    "    param_grid = {'C': [0.05, 1, 5, 10, 20, 40],\n",
    "              'gamma': [0.1 ,0.01, 0.005, 0.001, 0.0005, 0.0001],\n",
    "              'kernel': ['rbf','poly']}\n",
    "    grid = GridSearchCV(SVC(), param_grid, refit = True)\n",
    "    grid.fit(dataX, labelY)\n",
    "    print(\"Search for SVM hyperparameters complete\")\n",
    "    return grid.best_params_\n",
    "#This function trains SVM classifier\n",
    "def TrainSVM(imgFeatures, labelY):\n",
    "    params = SelectHyperparametersSVM(imgFeatures, labelY)\n",
    "    cParam, gammaParam, kernelType = params.get(\"C\"), params.get(\"gamma\"), params.get(\"kernel\")\n",
    "    print(cParam, gammaParam, kernelType)\n",
    "    svm = SVC(kernel = kernelType, C =  cParam, gamma = gammaParam, random_state=15)\n",
    "    svm.fit(imgFeatures, labelY)\n",
    "    return svm\n",
    "#This function trains Randomforest classifier\n",
    "def TrainRandomForestClassifier(ImgFeaturesX,labelsTrainY):\n",
    "    clf = RandomForestClassifier(n_estimators=100)\n",
    "    clf.fit(ImgFeaturesX,labelsTrainY)\n",
    "    return clf\n",
    "#This function is used for printing Confusion Matrix\n",
    "def DisplayConfusions(labelsY, predictions, classNames):\n",
    "    confMat = confusion_matrix(labelsY, predictions, labels = classNames)\n",
    "    print(confMat)\n",
    "    plt.rcParams[\"figure.figsize\"] = (14,7)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=confMat, display_labels=classNames)\n",
    "    disp.plot(xticks_rotation = 90)\n",
    "    plt.show()  \n",
    "#This function calculates the accuracy of the trained model for test dataset\n",
    "def CalculateAccuracy(labelsY, predictions):\n",
    "    print ('accuracy score: %f' % accuracy_score(labelsY, predictions))\n",
    "#This function display images    \n",
    "def DisplayImages(datasetX, index, predictions):\n",
    "    for i in range(len(index)):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        img = cv2.cvtColor(datasetX[index[i]], cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img)\n",
    "        plt.title(predictions[index[i]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036d136",
   "metadata": {},
   "source": [
    "# Train Model (Approach 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be913143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModelKP(datasetDirectoryName, noOfClusters, featureExtractorType, classifierType):\n",
    "    featureVectorLength = GetFeatureVectorLength(featureExtractorType)\n",
    "    featureDescriptorTrainList = []\n",
    "    print(\"Processing dataset....\")\n",
    "    datasetTrainX, labelsTrainY = BuildDataset(datasetDirectoryName, trainFlag = True)\n",
    "    print(\"Processing of dataset complete\")\n",
    "    print(\"Extracting Features.....\")\n",
    "    # Loop through complete dataset, extract feature vectors and stack them to form a feature vector stack \n",
    "    for i in range(len(datasetTrainX)):\n",
    "        featureDescriptorTrainList.append(ExtractFeatures(featureExtractorType, datasetTrainX[i]))\n",
    "    print(\"Feature Extraction complete\")\n",
    "    featDescriptorStackTrain = StackFeatureDescriptors(featureDescriptorTrainList)\n",
    "    print(\"Feature Descriptor dimensions:\"+str(featDescriptorStackTrain.shape))\n",
    "    print(\"Kmeans clustering ....\")\n",
    "    kmeans = ClusterofFeatures(featDescriptorStackTrain, noOfClusters)\n",
    "    print(\"Kmeans clustering complete\")\n",
    "    print(\"Constructing bag of visual words....\")\n",
    "    imgFeaturesTrain = ConstructBOVW(kmeans, featureDescriptorTrainList, len(datasetTrainX), noOfClusters, featureVectorLength)\n",
    "    print(\"Constructing bag of visual words complete\")\n",
    "    #PlotHistogram(imgFeaturesTrain, noOfClusters)\n",
    "    scaleObj, scaledImgFeaturesTrain = ScaleFeatures(imgFeaturesTrain)\n",
    "    print(\"Training Classifier....\")\n",
    "    if classifierType == 'SVM':\n",
    "        classifier = TrainSVM(scaledImgFeaturesTrain, labelsTrainY)\n",
    "    else:\n",
    "        classifier = TrainClassifier(scaledImgFeaturesTrain,labelsTrainY) # Random Forest Classifier\n",
    "    print(\"Training Classifier complete\")\n",
    "    return classifier, scaleObj, kmeans, scaledImgFeaturesTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab3470e",
   "metadata": {},
   "source": [
    "# Train Model (Approach 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModelHog(datasetDirectoryName, featureExtractorType, classifierType):\n",
    "    featureDescriptorTrainList = []\n",
    "    print(\"Processing dataset....\")\n",
    "    datasetTrainX, labelsTrainY = BuildDataset(datasetDirectoryName, trainFlag = True)\n",
    "    print(\"Processing of dataset complete\")\n",
    "    print(\"No of training images: \" + str(len(datasetTrainX)))\n",
    "    print(\"Extracting Features.....\")\n",
    "    # Loop through complete dataset, extract feature vectors and stack them to form a feature vector stack \n",
    "    for i in range(len(datasetTrainX)):\n",
    "        featureDescriptorTrainList.append(ExtractFeatures(featureExtractorType, datasetTrainX[i]))\n",
    "    featureDescriptorTrainArray = np.array(featureDescriptorTrainList)\n",
    "    print(\"Feature Extraction complete\")\n",
    "    print(\"Feature Descriptor dimensions:\"+str(featureDescriptorTrainArray.shape))\n",
    "    scaleObj, scaledImgFeatures = ScaleFeatures(featureDescriptorTrainArray)\n",
    "    pcaObj, pcaFeatures = ApplyPCA(scaledImgFeatures)\n",
    "    print(\"Feature Dimensions after PCA:\"+str(pcaFeatures.shape))\n",
    "    print(\"Training Classifier....\")\n",
    "    if classifierType == 'SVM':\n",
    "        classifier = classifier = TrainSVM(pcaFeatures, labelsTrainY)\n",
    "    else:\n",
    "        classifier = TrainClassifier(pcaFeatures,labelsTrainY) # Random Forest Classifier\n",
    "    print(\"Training of Classifier complete\")\n",
    "    return classifier, scaleObj, pcaObj, pcaFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7218c26",
   "metadata": {},
   "source": [
    "# Execute Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6703d25",
   "metadata": {},
   "source": [
    "1. Before executing this cell, import requisite modules and execute the supporting functions.\n",
    "2. It is also assumed that dataset folder \"CS893 Sp2022 A1 Dataset\" is present in same folder.\n",
    "3. For Approach 1 change {featureExtractorType = \"SIFT\"}\n",
    "4. For Approach 2 change {featureExtractorType = \"HOG\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7977df8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasetDirectoryName = \"CS893 Sp2022 A1 Dataset\"\n",
    "noOfClusters = 500 # No of clusters for Kmeans\n",
    "featureExtractorType = \"HOG\" # Options include SIFT, HOG\n",
    "classifierType = \"SVM\" # Options include SVM, RANDOMFOREST\n",
    "if featureExtractorType == \"HOG\": \n",
    "    classifier, scaleObj, pcaObj, imgFeaturesTrain = TrainModelHog(datasetDirectoryName, featureExtractorType, classifierType)\n",
    "else:\n",
    "    classifier, scaleObj, kmeans, imgFeaturesTrain = TrainModelKP(datasetDirectoryName, noOfClusters, featureExtractorType, classifierType)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a0593",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ea0f44",
   "metadata": {},
   "source": [
    "# Test Model Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe09ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestModelKP(datasetDirectoryName, noOfClusters, featureExtractorType, classifierType):\n",
    "    featureVectorLength = GetFeatureVectorLength(featureExtractorType)\n",
    "    featureDescriptorTestList = []\n",
    "    print(\"Processing dataset started....\")\n",
    "    datasetTestX, labelsTestY = BuildDataset(datasetDirectoryName, trainFlag = False)\n",
    "    print(\"Processing of dataset completed....\")\n",
    "    print(\"No of test images: \" + str(len(datasetTestX)))\n",
    "    print(\"Extracting Features.....\")\n",
    "    # Loop through complete dataset, extract feature vectors and stack them to form a feature vector stack \n",
    "    for i in range(len(datasetTestX)):\n",
    "        featureDescriptorTestList.append(ExtractFeatures(featureExtractorType, datasetTestX[i]))\n",
    "    print(\"Feature Extraction complete\")\n",
    "    print(\"Constructing Bag of Visual Words....\")\n",
    "    imgFeaturesTest = ConstructBOVW(kmeans, featureDescriptorTestList, len(datasetTestX), noOfClusters, featureVectorLength)\n",
    "    print(\"Constructing Bag of Visual Words complete\")\n",
    "    scaledImgFeaturesTest = scaleObj.transform(imgFeaturesTest)\n",
    "    print(\"Predicting on test dataset....\")\n",
    "    if classifierType == 'SVM':\n",
    "        predictions = classifier.predict(scaledImgFeaturesTest)\n",
    "    else:\n",
    "        predictions = classifier.predict(scaledImgFeaturesTest) # Random Forest Classifier\n",
    "    return labelsTestY, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c72bfc",
   "metadata": {},
   "source": [
    "# Test Model Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2debca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestModelHOG(datasetDirectoryName, featureExtractorType, classifierType):\n",
    "    featureDescriptorTestList = []\n",
    "    print(\"Processing dataset started....\")\n",
    "    datasetTestX, labelsTestY = BuildDataset(datasetDirectoryName, trainFlag = False)\n",
    "    print(\"Processing of dataset completed\")\n",
    "    print(\"No of test images: \" + str(len(datasetTestX)))\n",
    "    print(\"Extracting Features.....\")\n",
    "    # Loop through complete dataset, extract feature vectors and stack them to form a feature vector stack \n",
    "    for i in range(len(datasetTestX)):\n",
    "        featureDescriptorTestList.append(ExtractFeatures(featureExtractorType, datasetTestX[i]))\n",
    "    \n",
    "    featureDescriptorTestArray = np.array(featureDescriptorTestList)\n",
    "    print(\"Feature Extraction complete\")\n",
    "    print(\"Feature Descriptor dimensions:\"+str(featureDescriptorTestArray.shape))\n",
    "    scaledImgFeaturesTest = scaleObj.transform(featureDescriptorTestArray)\n",
    "    pcaFeaturesTest = pcaObj.transform(scaledImgFeaturesTest)\n",
    "    print(\"Feature Dimensions after PCA:\"+str(pcaFeaturesTest.shape))\n",
    "    print(\"Predicting on test dataset....\")\n",
    "    if classifierType == 'SVM':\n",
    "        predictions = classifier.predict(pcaFeaturesTest)\n",
    "    else:\n",
    "        predictions = classifier.predict(pcaFeaturesTest) # Random Forest Classifier\n",
    "    return labelsTestY, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd01f0",
   "metadata": {},
   "source": [
    "# Execute Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51494ed6",
   "metadata": {},
   "source": [
    "1. Before executing this cell, make sure that training for respective model has been executed.\n",
    "2. It is also assumed that dataset folder \"CS893 Sp2022 A1 Dataset\" is present in same folder.\n",
    "3. For Approach 1 change {featureExtractorType = \"SIFT\"}\n",
    "4. For Approach 2 change {featureExtractorType = \"HOG\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c96024",
   "metadata": {},
   "outputs": [],
   "source": [
    "classNames = [\"Hump\", \"Limit 70\", \"Limit 50\", \"Children crossing\", \"Forbidden Direction\", \"Give way\", \"No entry\", \"Bicycle way\", \"Bicyle & children way\", \"No Parking\"]\n",
    "datasetDirectoryName = \"CS893 Sp2022 A1 Dataset\"\n",
    "noOfClusters = 500\n",
    "featureExtractorType = \"HOG\"\n",
    "classifierType = \"SVM\"\n",
    "if featureExtractorType == \"HOG\":\n",
    "    labelsTestY, predictions = TestModelHOG(datasetDirectoryName, featureExtractorType, classifierType)\n",
    "else:\n",
    "    labelsTestY, predictions = TestModelKP(datasetDirectoryName, noOfClusters, featureExtractorType, classifierType)\n",
    "print(\"Confusion Matrix for test dataset\")\n",
    "labelsTestY = np.array(labelsTestY)\n",
    "DisplayConfusions(labelsTestY, predictions, classNames)\n",
    "CalculateAccuracy(labelsTestY, predictions)\n",
    "classificationReport = classification_report(labelsTestY,predictions,labels=classNames)\n",
    "print(classificationReport)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd968c24",
   "metadata": {},
   "source": [
    "# Print Wrongly Classified Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8509a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = labelsTestY==predictions\n",
    "b = np.where(a==False)\n",
    "wronglyClassifiedIndexes = b[0]\n",
    "\n",
    "datasetX, labelsY = ReadDataset(datasetDirectoryName, trainFlag = False)\n",
    "print(\"Wronly Classified Images\")\n",
    "DisplayImages(datasetX, wronglyClassifiedIndexes, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba484f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
